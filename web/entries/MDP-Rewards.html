<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Markov Decision Processes with Rewards - Archive of Formal Proofs</title>
    <meta name="description" content="Markov Decision Processes with Rewards in the Archive of Formal Proofs">
    <meta property="og:description" content="Markov Decision Processes with Rewards in the Archive of Formal Proofs">
    <meta property="og:title" content="Markov Decision Processes with Rewards">
    <meta property="og:url" content="https://isa-afp.org/entries/MDP-Rewards.html">
    <meta property="og:image" content="https://isa-afp.org/images/afp.png">
    <meta property="og:type" content="article">
  <link rel="stylesheet" type="text/css" href="../css/front.min.css">
  <link rel="icon" href="../images/favicon.ico" type="image/icon">
    
    <script>
      MathJax = {
        tex: {
          inlineMath: [["$", "$"], ["\\(", "\\)"]]
        },
        processEscapes: true,
        svg: {
          fontCache: "global"
        }
      };
    </script>
    <script id="MathJax-script" async src="../js/mathjax/es5/tex-mml-chtml.js">
    </script>

  <script src="../js/util.js"></script>
  <script src="../js/flexsearch.bundle.js"></script>
  <script src="../js/search-autocomplete.js"></script>
    <script src="../js/obfuscate.js"></script>
    <script src="../js/entries.js"></script>
</head>

  <body class="mathjax_ignore">
    <aside><div id="menu-toggle">
  <input id="toggle" type="checkbox">
  <label for="toggle">
    <span>menu</span>
    <img src="../images/menu.svg" alt="Menu">
  </label>
  <a href="../" class="logo-link">
    <img src="../images/afp.png" alt="Logo of the Archive of Formal Proofs" class="logo">
  </a>
  <nav id="menu">
    <div>
      <a href="../" class="logo-link">
        <img src="../images/afp.png" alt="Logo of the Archive of Formal Proofs" class="logo">
      </a>
      <ul>
          <li >
            <a href="../">Home</a>
          </li>
          <li >
            <a href="../topics/">Topics</a>
          </li>
          <li >
            <a href="../download/">Download</a>
          </li>
          <li >
            <a href="../help/">Help</a>
          </li>
          <li >
            <a href="../submission/">Submission</a>
          </li>
          <li >
            <a href="../statistics/">Statistics</a>
          </li>
          <li >
            <a href="../about/">About</a>
          </li>
      </ul>
    </div>
  </nav>
</div>
    </aside>

    <div class="content entries"><header>
    <form autocomplete="off" action="../search/">
      <div class="form-container">
        <input id="search-input" name="s" type="search" size="31" maxlength="255" value=""
               aria-label="Search the AFP" list="autocomplete"><button id="search-button" type="submit">
          <img src="../images/search.svg" alt="Search">
        </button>
        <datalist id="autocomplete"></datalist>
      </div>
    </form>
  <h1>
    <span class='first'>M</span>arkov <span class='first'>D</span>ecision <span class='first'>P</span>rocesses with <span class='first'>R</span>ewards
  </h1>
  <div>
      <p><a href="../authors/schaeffeler/">Maximilian SchÃ¤ffeler</a> <a class="obfuscated" data="eyJ1c2VyIjpbInNjaGFlZmZtIl0sImhvc3QiOlsiaW4iLCJ0dW0iLCJkZSJdfQ==">ðŸ“§</a> and <a href="../authors/abdulaziz/">Mohammad Abdulaziz</a> <a class="obfuscated" data="eyJ1c2VyIjpbIm1vaGFtbWFkIiwiYWJkdWxheml6Il0sImhvc3QiOlsiaW4iLCJ0dW0iLCJkZSJdfQ==">ðŸ“§</a>
      </p>
      <p class="date">December 16, 2021</p>
  </div>
</header>
      <div>
  <main>

    <h3>Abstract</h3>
    <div class="abstract mathjax_process">We present a formalization of Markov Decision Processes with rewards.
In particular we first build on HÃ¶lzl's formalization  of MDPs
(AFP entry: Markov_Models) and extend them with rewards. We proceed
with an analysis of the expected total discounted reward criterion for
infinite horizon MDPs. The central result is the construction of the
iteration rule for the Bellman operator. We prove the optimality
equations for this operator and show the existence of an optimal
stationary deterministic solution. The analysis can be used to obtain
dynamic programming algorithms such as value iteration and policy
iteration to solve MDPs with formal guarantees. Our formalization is
based on chapters 5 and 6 in Puterman's book "Markov
Decision Processes: Discrete Stochastic Dynamic Programming".</div>

    <h3>License</h3>
    <div>
        <a href="https://isa-afp.org/LICENSE">BSD License</a>
    </div>
      <h3>Topics</h3>
      <ul>
          <li><a href="../topics/mathematics/probability-theory/">Mathematics/Probability theory</a></li>
      </ul>
      <h3>Session MDP-Rewards</h3>
      <ul>
          <li><a href="../thys/AFP/MDP-Rewards/Bounded_Functions.html">Bounded_Functions</a></li>
          <li><a href="../thys/AFP/MDP-Rewards/Blinfun_Util.html">Blinfun_Util</a></li>
          <li><a href="../thys/AFP/MDP-Rewards/MDP_reward_Util.html">MDP_reward_Util</a></li>
          <li><a href="../thys/AFP/MDP-Rewards/MDP_cont.html">MDP_cont</a></li>
          <li><a href="../thys/AFP/MDP-Rewards/MDP_disc.html">MDP_disc</a></li>
          <li><a href="../thys/AFP/MDP-Rewards/MDP_reward.html">MDP_reward</a></li>
      </ul>

    <div class="flex-wrap">
        <div>
          <h3>Used by</h3>
          <ul class="horizontal-list">
              <li><a href="../entries/MDP-Algorithms.html">Verified Algorithms for Solving Markov Decision Processes</a></li>
          </ul>
        </div>
    </div>
  </main>

  <nav class="links">
    <a class="popup-button" href="#cite-popup">Cite</a>
    <a class="popup-button" href="#download-popup">Download</a>
    <h4>PDFs</h4>
    <a href="https://www.isa-afp.org/browser_info/current/AFP/MDP-Rewards/outline.pdf">Proof outline</a>
    <a href="https://www.isa-afp.org/browser_info/current/AFP/MDP-Rewards/document.pdf">Proof document</a>
    <a href="https://www.isa-afp.org/browser_info/current/AFP/MDP-Rewards/session_graph.pdf">Dependencies</a>
  </nav>

  <div id="cite-popup" class="overlay">
    <a class="cancel" href="#"></a>
    <div class="popup">
      <h2>Cite</h2>
      <a class="close" href="#">&times;</a>
      <div>
        <p style="display:none;" id="bibtex-filename">MDP-Rewards-AFP</p>
        <pre id="copy-text">@article{MDP-Rewards-AFP,
  author  = {Maximilian SchÃ¤ffeler and Mohammad Abdulaziz},
  title   = {Markov Decision Processes with Rewards},
  journal = {Archive of Formal Proofs},
  month   = {December},
  year    = {2021},
  note    = {\url{https://isa-afp.org/entries/MDP-Rewards.html},
             Formal proof development},
  ISSN    = {2150-914x},
}</pre>
        <button id="copy-bibtex">Copy</button> <a id="download-bibtex">Download</a>
      </div>
    </div>
  </div>

  <div id="download-popup" class="overlay">
    <a class="cancel" href="#"></a>
    <div class="popup">
      <h2>Download</h2>
      <a class="close" href="#">&times;</a>
      <a href="https://www.isa-afp.org/release/afp-MDP-Rewards-current.tar.gz" download>
        Download latest</a>
        <p>Older releases:</p>
        <ul>
            <li>
              <a href="https://www.isa-afp.org/release/afp-MDP-Rewards-2026-02-06.tar.gz">
                Feb 6, 2026
              </a>
              : Isabelle2025-2
            </li>
            <li>
              <a href="https://www.isa-afp.org/release/afp-MDP-Rewards-2025-12-22.tar.gz">
                Dec 22, 2025
              </a>
              : Isabelle2025-1
            </li>
            <li>
              <a href="https://www.isa-afp.org/release/afp-MDP-Rewards-2025-03-17.tar.gz">
                Mar 17, 2025
              </a>
              : Isabelle2025
            </li>
            <li>
              <a href="https://www.isa-afp.org/release/afp-MDP-Rewards-2024-05-26.tar.gz">
                May 26, 2024
              </a>
              : Isabelle2024
            </li>
            <li>
              <a href="https://www.isa-afp.org/release/afp-MDP-Rewards-2023-09-13.tar.gz">
                Sep 13, 2023
              </a>
              : Isabelle2023
            </li>
            <li>
              <a href="https://www.isa-afp.org/release/afp-MDP-Rewards-2022-10-27.tar.gz">
                Oct 27, 2022
              </a>
              : Isabelle2022
            </li>
            <li>
              <a href="https://www.isa-afp.org/release/afp-MDP-Rewards-2021-12-28.tar.gz">
                Dec 28, 2021
              </a>
              : Isabelle2021-1
            </li>
        </ul>
    </div>
  </div>
      </div>
    </div>
  </body>
</html>